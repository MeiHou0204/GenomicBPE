{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41f38bb7",
   "metadata": {},
   "source": [
    "## To create BPE vocabulary from DNA sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566b1355",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d39957",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 55 sequences\n",
      "Merges: 100, Vocab size: 104, Last merge: ('G', 'TT') -> GTT (freq: 37)\n",
      "Merges: 200, Vocab size: 204, Last merge: ('TGGTTACCTGCAACCGGTAAAGTATATCTACCACCATCGACCCCAGTTGCAAGGGTACAAAGCACGGATGAGTACATACAAAGGACTGACATCTTTTACCACGCTAATAGTGATCGGTTACTCACAGTAGGACATCCATACTTTGAGGTTCGAGCCACAACAGAACCATATCAGGTGACAGTACCTAAAGTTAGTGGAAATCAGTTTAGAGCTTTCAGACTTAAATTACCTGATCCTAATAGGTTTGCATTGGTAGATACTACAGTGTATAATCCTGACAAGGAAAGATTAGTC', 'TGGG') -> TGGTTACCTGCAACCGGTAAAGTATATCTACCACCATCGACCCCAGTTGCAAGGGTACAAAGCACGGATGAGTACATACAAAGGACTGACATCTTTTACCACGCTAATAGTGATCGGTTACTCACAGTAGGACATCCATACTTTGAGGTTCGAGCCACAACAGAACCATATCAGGTGACAGTACCTAAAGTTAGTGGAAATCAGTTTAGAGCTTTCAGACTTAAATTACCTGATCCTAATAGGTTTGCATTGGTAGATACTACAGTGTATAATCCTGACAAGGAAAGATTAGTCTGGG (freq: 25)\n",
      "Merges: 300, Vocab size: 304, Last merge: ('TACTGGGTACATTACCAATTCCAAGGATGACAGACAAGATACATCCTTTGATCCCAAACAGGTACAAATGTTTATAATTGGCTGTACCCCTTGCTGGGGAGAGCATTGGGATATTGCTCCACGCTGTGATGATGATCAACCTATCCAAGGGGCCTGTCCTCCATTAGAATTAAGAAATACTATTATTGAGGATGGCGATATGGTAGACATAGGTTTTGGTAATATAAATAACAAAACTCTTTCAGTGAC', 'CAA') -> TACTGGGTACATTACCAATTCCAAGGATGACAGACAAGATACATCCTTTGATCCCAAACAGGTACAAATGTTTATAATTGGCTGTACCCCTTGCTGGGGAGAGCATTGGGATATTGCTCCACGCTGTGATGATGATCAACCTATCCAAGGGGCCTGTCCTCCATTAGAATTAAGAAATACTATTATTGAGGATGGCGATATGGTAGACATAGGTTTTGGTAATATAAATAACAAAACTCTTTCAGTGACCAA (freq: 25)\n",
      "Merges: 400, Vocab size: 404, Last merge: ('AAAAGACATTAGGTAGTTCTATTTATGTGCCCACTGTAAGTGGCTCTTTAGTGTCTTCTGATGCACAGCTATTTAATAGGCCTTTTTGGTTACAAAGGGCACAAGGACACAACAATGGCATATGCTGGGAAAATCAGCTTTTTGTCACTGTTGTAGATAACACACGCAATACTAACTTTACAATTAGTGTGTCTTCCACTGATCAAGCACCTACAGAGTATAATGCTAGTAATACTCCTAATTTTAGGGAATATTTAAGACATGTAGAGGAATATGAGCTCTCTGTC', 'ATT') -> AAAAGACATTAGGTAGTTCTATTTATGTGCCCACTGTAAGTGGCTCTTTAGTGTCTTCTGATGCACAGCTATTTAATAGGCCTTTTTGGTTACAAAGGGCACAAGGACACAACAATGGCATATGCTGGGAAAATCAGCTTTTTGTCACTGTTGTAGATAACACACGCAATACTAACTTTACAATTAGTGTGTCTTCCACTGATCAAGCACCTACAGAGTATAATGCTAGTAATACTCCTAATTTTAGGGAATATTTAAGACATGTAGAGGAATATGAGCTCTCTGTCATT (freq: 25)\n",
      "Merges: 500, Vocab size: 504, Last merge: ('AAACGGTCTGCTACACCTAAAACCACAAC', 'AGCTC') -> AAACGGTCTGCTACACCTAAAACCACAACAGCTC (freq: 19)\n",
      "Stopping: no token frequency > 5\n",
      "Training completed. Final vocab size: 596\n",
      "Total merges: 592\n",
      "\n",
      "First 20 tokens in vocabulary:\n",
      "   1. 'G' (length: 1)\n",
      "   2. 'T' (length: 1)\n",
      "   3. 'C' (length: 1)\n",
      "   4. 'A' (length: 1)\n",
      "   5. 'TT' (length: 2)\n",
      "   6. 'AG' (length: 2)\n",
      "   7. 'GA' (length: 2)\n",
      "   8. 'TC' (length: 2)\n",
      "   9. 'GG' (length: 2)\n",
      "  10. 'CC' (length: 2)\n",
      "  11. 'GC' (length: 2)\n",
      "  12. 'TA' (length: 2)\n",
      "  13. 'AA' (length: 2)\n",
      "  14. 'TG' (length: 2)\n",
      "  15. 'AC' (length: 2)\n",
      "  16. 'GAC' (length: 3)\n",
      "  17. 'CAC' (length: 3)\n",
      "  18. 'CCC' (length: 3)\n",
      "  19. 'AAC' (length: 3)\n",
      "  20. 'GAA' (length: 3)\n",
      "\n",
      "==================================================\n",
      "Compression Analysis\n",
      "==================================================\n",
      "Seq 1: Original=738, Tokens=275, Compression=2.68x\n",
      "Seq 2: Original=603, Tokens=222, Compression=2.72x\n",
      "Seq 3: Original=747, Tokens=277, Compression=2.70x\n",
      "Seq 4: Original=603, Tokens=222, Compression=2.72x\n",
      "Seq 5: Original=747, Tokens=277, Compression=2.70x\n",
      "Seq 6: Original=603, Tokens=222, Compression=2.72x\n",
      "Seq 7: Original=747, Tokens=277, Compression=2.70x\n",
      "Seq 8: Original=603, Tokens=222, Compression=2.72x\n",
      "Seq 9: Original=747, Tokens=277, Compression=2.70x\n",
      "Seq 10: Original=603, Tokens=223, Compression=2.70x\n",
      "Seq 11: Original=759, Tokens=280, Compression=2.71x\n",
      "Seq 12: Original=789, Tokens=286, Compression=2.76x\n",
      "Seq 13: Original=747, Tokens=277, Compression=2.70x\n",
      "Seq 14: Original=792, Tokens=287, Compression=2.76x\n",
      "Seq 15: Original=747, Tokens=277, Compression=2.70x\n",
      "Seq 16: Original=705, Tokens=256, Compression=2.75x\n",
      "Seq 17: Original=747, Tokens=277, Compression=2.70x\n",
      "Seq 18: Original=789, Tokens=286, Compression=2.76x\n",
      "Seq 19: Original=747, Tokens=277, Compression=2.70x\n",
      "Seq 20: Original=783, Tokens=287, Compression=2.73x\n",
      "Seq 21: Original=729, Tokens=273, Compression=2.67x\n",
      "Seq 22: Original=627, Tokens=231, Compression=2.71x\n",
      "Seq 23: Original=741, Tokens=276, Compression=2.68x\n",
      "Seq 24: Original=627, Tokens=231, Compression=2.71x\n",
      "Seq 25: Original=741, Tokens=276, Compression=2.68x\n",
      "Seq 26: Original=627, Tokens=231, Compression=2.71x\n",
      "Seq 27: Original=741, Tokens=276, Compression=2.68x\n",
      "Seq 28: Original=627, Tokens=231, Compression=2.71x\n",
      "Seq 29: Original=741, Tokens=276, Compression=2.68x\n",
      "Seq 30: Original=627, Tokens=232, Compression=2.70x\n",
      "Seq 31: Original=768, Tokens=280, Compression=2.74x\n",
      "Seq 32: Original=672, Tokens=251, Compression=2.68x\n",
      "Seq 33: Original=756, Tokens=277, Compression=2.73x\n",
      "Seq 34: Original=672, Tokens=251, Compression=2.68x\n",
      "Seq 35: Original=756, Tokens=277, Compression=2.73x\n",
      "Seq 36: Original=648, Tokens=240, Compression=2.70x\n",
      "Seq 37: Original=756, Tokens=277, Compression=2.73x\n",
      "Seq 38: Original=672, Tokens=251, Compression=2.68x\n",
      "Seq 39: Original=756, Tokens=277, Compression=2.73x\n",
      "Seq 40: Original=672, Tokens=252, Compression=2.67x\n",
      "\n",
      "Overall: Average compression ratio = 2.71x\n",
      "Reduced from 28302 to 10450 tokens (36.9% of original)\n",
      "\n",
      "Vocabulary saved to bpe_vocab.txt\n"
     ]
    }
   ],
   "source": [
    "class DNABPE:\n",
    "    def __init__(self, vocab_size=4000, min_frequency=3):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.min_frequency = min_frequency\n",
    "        self.vocab = set()\n",
    "        self.merges = {}\n",
    "        \n",
    "    def get_stats(self, sequences):\n",
    "        \"\"\"computing frequency of adjacent symbol pairs\"\"\"\n",
    "        pairs = defaultdict(int)\n",
    "        for sequence in sequences:\n",
    "            symbols = sequence.split()\n",
    "            for i in range(len(symbols) - 1):\n",
    "                pairs[symbols[i], symbols[i + 1]] += 1\n",
    "        return pairs\n",
    "    \n",
    "    def merge_vocab(self, pair, sequences):\n",
    "        \"\"\"merge the most frequent pair in the sequences\"\"\"\n",
    "        first, second = pair\n",
    "        new_sequences = []\n",
    "        pattern = re.compile(r'(?<!\\S)' + re.escape(first + ' ' + second) + r'(?!\\S)')\n",
    "        replacement = first + second\n",
    "        \n",
    "        for sequence in sequences:\n",
    "            new_sequence = pattern.sub(replacement, sequence)\n",
    "            new_sequences.append(new_sequence)\n",
    "            \n",
    "        return new_sequences, replacement\n",
    "    \n",
    "    def fit(self, fasta_file):\n",
    "        \"\"\"training BPE from a FASTA file\"\"\"\n",
    "        #loading fasta sequences\n",
    "        sequences = []\n",
    "        with open(fasta_file, 'r') as f:\n",
    "            current_seq = \"\"\n",
    "            for line in f:\n",
    "                if line.startswith('>'):\n",
    "                    if current_seq:\n",
    "                        sequences.append(current_seq)\n",
    "                    current_seq = \"\"\n",
    "                else:\n",
    "                    current_seq += line.strip()\n",
    "            if current_seq:\n",
    "                sequences.append(current_seq)\n",
    "        \n",
    "        print(f\"Loaded {len(sequences)} sequences\")\n",
    "        \n",
    "        # initial tokens with single nucleotides\n",
    "        initial_vocab = {'A', 'T', 'C', 'G'}\n",
    "        self.vocab = initial_vocab.copy()\n",
    "        \n",
    "        # transform sequences to tokenized form\n",
    "        tokenized_seqs = [' '.join(list(seq)) for seq in sequences]\n",
    "        \n",
    "        # traning loop\n",
    "        num_merges = 0\n",
    "        while len(self.vocab) < self.vocab_size:\n",
    "            # stats: { (token1, token2): frequency }\n",
    "            stats = self.get_stats(tokenized_seqs)\n",
    "            if not stats:\n",
    "                break\n",
    "                \n",
    "            # find the most frequent pair\n",
    "            best_pair = max(stats.items(), key=lambda x: x[1])\n",
    "            pair, freq = best_pair\n",
    "            \n",
    "            # check stopping cutoff\n",
    "            if freq < self.min_frequency:\n",
    "                print(f\"Stopping: no token frequency > {self.min_frequency}\")\n",
    "                break\n",
    "                \n",
    "            # update vocab\n",
    "            tokenized_seqs, new_token = self.merge_vocab(pair, tokenized_seqs)\n",
    "            self.vocab.add(new_token)\n",
    "            self.merges[pair] = new_token\n",
    "            num_merges += 1\n",
    "            \n",
    "            if num_merges % 100 == 0:\n",
    "                print(f\"Merges: {num_merges}, Vocab size: {len(self.vocab)}, \"\n",
    "                      f\"Last merge: {pair} -> {new_token} (freq: {freq})\")\n",
    "        \n",
    "        print(f\"Training completed. Final vocab size: {len(self.vocab)}\")\n",
    "        print(f\"Total merges: {num_merges}\")\n",
    "        return tokenized_seqs\n",
    "    \n",
    "    def tokenize(self, sequence):\n",
    "        \"\"\" tokenize a single sequence\"\"\"\n",
    "        tokens = list(sequence)  # initial tokens are single characters\n",
    "        \n",
    "        # apply BPE tokenization\n",
    "        changed = True\n",
    "        while changed:\n",
    "            changed = False\n",
    "            i = 0\n",
    "            new_tokens = []\n",
    "            \n",
    "            while i < len(tokens):\n",
    "                if i < len(tokens) - 1:\n",
    "                    pair = (tokens[i], tokens[i + 1])\n",
    "                    if pair in self.merges:\n",
    "                        new_tokens.append(self.merges[pair])\n",
    "                        i += 2\n",
    "                        changed = True\n",
    "                    else:\n",
    "                        new_tokens.append(tokens[i])\n",
    "                        i += 1\n",
    "                else:\n",
    "                    new_tokens.append(tokens[i])\n",
    "                    i += 1\n",
    "                    \n",
    "            tokens = new_tokens\n",
    "            \n",
    "        return tokens\n",
    "    \n",
    "    def analyze_compression(self, fasta_file):\n",
    "        \"\"\"analyze compression ratio\"\"\"\n",
    "        # reload sequences for compression analysis\n",
    "        sequences = []\n",
    "        with open(fasta_file, 'r') as f:\n",
    "            current_seq = \"\"\n",
    "            for line in f:\n",
    "                if line.startswith('>'):\n",
    "                    if current_seq:\n",
    "                        sequences.append(current_seq)\n",
    "                    current_seq = \"\"\n",
    "                else:\n",
    "                    current_seq += line.strip()\n",
    "            if current_seq:\n",
    "                sequences.append(current_seq)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"Compression Analysis\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        total_original_len = 0\n",
    "        total_token_len = 0\n",
    "        \n",
    "        for i, seq in enumerate(sequences[:40]):  # only analyze first 40 sequences\n",
    "            tokens = self.tokenize(seq)\n",
    "            original_len = len(seq)\n",
    "            token_len = len(tokens)\n",
    "            compression_ratio = original_len / token_len\n",
    "            \n",
    "            total_original_len += original_len\n",
    "            total_token_len += token_len\n",
    "            \n",
    "            print(f\"Seq {i+1}: Original={original_len}, \"\n",
    "                  f\"Tokens={token_len}, Compression={compression_ratio:.2f}x\")\n",
    "        \n",
    "        # summary\n",
    "        if sequences:\n",
    "            avg_compression = total_original_len / total_token_len\n",
    "            print(f\"\\nOverall: Average compression ratio = {avg_compression:.2f}x\")\n",
    "            print(f\"Reduced from {total_original_len} to {total_token_len} tokens \"\n",
    "                  f\"({total_token_len/total_original_len*100:.1f}% of original)\")\n",
    "        \n",
    "        return {\n",
    "            'vocab_size': len(self.vocab),\n",
    "            'merges': len(self.merges),\n",
    "            'avg_compression_ratio': avg_compression if sequences else 0\n",
    "        }\n",
    "\n",
    "def main():\n",
    "    # usage example\n",
    "    bpe = DNABPE(vocab_size=4000, min_frequency=5)\n",
    "    \n",
    "    # BPE training\n",
    "    tokenized_sequences = bpe.fit(\"./data/test3.fasta\")  # 替换为你的FASTA文件\n",
    "    \n",
    "    # show part of the vocabulary\n",
    "    print(f\"\\nFirst 20 tokens in vocabulary:\")\n",
    "    vocab_list = sorted(list(bpe.vocab), key=len)\n",
    "    for i, token in enumerate(vocab_list[:20]):\n",
    "        print(f\"  {i+1:2d}. '{token}' (length: {len(token)})\")\n",
    "    \n",
    "    # analyzing compression ratio\n",
    "    stats = bpe.analyze_compression(\"./data/test3.fasta\")\n",
    "    \n",
    "    # save vocabulary to file\n",
    "    with open(\"bpe_vocab.txt\", \"w\") as f:\n",
    "        for token in sorted(bpe.vocab, key=len):\n",
    "            f.write(f\"{token}\\n\")\n",
    "    \n",
    "    print(f\"\\nVocabulary saved to bpe_vocab.txt\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlearn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
